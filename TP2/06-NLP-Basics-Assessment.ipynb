{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ère partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL to perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Créer un objet Doc à partir du fichier `owlcreek.txt`**<br>\n",
    "> HINT: Use `with open('../TextFiles/owlcreek.txt') as f:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "with open('../TextFiles/owlcreek.txt', encoding='unicode_escape') as f:\n",
    "    doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
       "\n",
       "by Ambrose Bierce\n",
       "\n",
       "I\n",
       "\n",
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the swift water twenty feet below.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to verify it worked:\n",
    "doc[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Combien de tokens sont contenus dans le fichier ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4833"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Combien de phrases sont contenues dans le fichier ?<br>HINT: Vous devez d'abord créer une liste !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = []\n",
    "for sent in doc.sents:\n",
    "    sents.append(sent)\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Imprimer la deuxième phrase du document**<br> HINT: L'indexation commence à zéro et le titre compte comme la première phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the swift water twenty feet below.  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Pour chaque élément de la phrase ci-dessus, imprimez son`text`, `POS` tag, `dep` tag and `lemma`<br>\n",
    "CHALLENGE: Faire en sorte que les valeurs s'alignent en colonnes dans la sortie imprimée.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DET det a\n",
      "man NOUN nsubj man\n",
      "stood VERB ROOT stand\n",
      "upon ADP prep upon\n",
      "a DET det a\n",
      "railroad NOUN compound railroad\n",
      "bridge NOUN pobj bridge\n",
      "in ADP prep in\n",
      "northern ADJ amod northern\n",
      "Alabama PROPN pobj alabama\n",
      ", PUNCT punct ,\n",
      "looking VERB advcl look\n",
      "down PART prt down\n",
      "\n",
      " SPACE  \n",
      "\n",
      "into ADP prep into\n",
      "the DET det the\n",
      "swift ADJ amod swift\n",
      "water NOUN pobj water\n",
      "twenty NUM nummod twenty\n",
      "feet NOUN npadvmod foot\n",
      "below ADV advmod below\n",
      ". PUNCT punct .\n",
      "  SPACE   \n"
     ]
    }
   ],
   "source": [
    "# NORMAL SOLUTION:\n",
    "for token in sents[1]:\n",
    "    print(f'{token.text} {token.pos_} {token.dep_} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A               DET      det      a\n",
      "man             NOUN     nsubj    man\n",
      "stood           VERB     ROOT     stand\n",
      "upon            ADP      prep     upon\n",
      "a               DET      det      a\n",
      "railroad        NOUN     compound railroad\n",
      "bridge          NOUN     pobj     bridge\n",
      "in              ADP      prep     in\n",
      "northern        ADJ      amod     northern\n",
      "Alabama         PROPN    pobj     alabama\n",
      ",               PUNCT    punct    ,\n",
      "looking         VERB     advcl    look\n",
      "down            PART     prt      down\n",
      "\n",
      "               SPACE             \n",
      "\n",
      "into            ADP      prep     into\n",
      "the             DET      det      the\n",
      "swift           ADJ      amod     swift\n",
      "water           NOUN     pobj     water\n",
      "twenty          NUM      nummod   twenty\n",
      "feet            NOUN     npadvmod foot\n",
      "below           ADV      advmod   below\n",
      ".               PUNCT    punct    .\n",
      "                SPACE              \n"
     ]
    }
   ],
   "source": [
    "# CHALLENGE SOLUTION:\n",
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{15}} {token.pos_:{8}} {token.dep_:<{8}} {token.lemma_}')\n",
    "show_lemmas(sents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ème partie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1: Tokenisation de phrases avec SpaCy\n",
    "\n",
    "Question : Créez une phrase complexe en anglais et utilisez SpaCy pour la tokeniser en phrases individuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Tokenization is a fundamental step in NLP. It involves breaking down a text into smaller units called tokens.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is a fundamental step in NLP.', 'It involves breaking down a text into smaller units called tokens.']\n"
     ]
    }
   ],
   "source": [
    "ex1 = [str(sent) for sent in nlp(sent).sents]\n",
    "print(ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Utilisez SpaCy pour tokeniser la phrase complexe en mots.\n",
    "\n",
    "Question : Quels avantages offre SpaCy par rapport à d'autres bibliothèques pour la tokenisation?\n",
    "\n",
    "Question : Créez une fonction custom_spacy_tokenizer qui prend une phrase en entrée, utilise SpaCy pour la tokeniser et renvoie les tokens en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_tokenizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'nlp', '.', 'it', 'involves', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "def custom_spacy_tokenizer(sent):\n",
    "    return [str(sent).lower() for sent in nlp(sent)]\n",
    "\n",
    "print(custom_spacy_tokenizer(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3\n",
    "\n",
    "Question : Utilisez SpaCy pour extraire les lemmes et les POS de la phrase complexe.\n",
    "\n",
    "Question : Comment SpaCy gère-t-il la tokenisation des entités nommées? Testez cela sur une phrase contenant une entité nommée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entity: Apple, Label: ORG\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation des entités nommées avec SpaCy\n",
    "text_with_entity = \"Apple is a major technology company.\"\n",
    "lem = nlp(text_with_entity)\n",
    "\n",
    "for token in lem.ents:\n",
    "    print(f'Named entity: {token.text}, Label: {token.label_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer le stemming sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Stemming', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'reducing', 'words', 'to', 'their', 'base', 'form', '.']\n",
      "\n",
      "\n",
      "Stems: ['stem', 'is', 'an', 'essenti', 'part', 'of', 'natur', 'languag', 'process', '.', 'it', 'involv', 'reduc', 'word', 'to', 'their', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "# Création d'une phrase complexe\n",
    "text = \"Stemming is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "sent = nlp(text)\n",
    "original = [str(word) for word in sent]\n",
    "print(f'Original words: {original}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "stemming = [s_stemmer.stem(str(word)) for word in sent]\n",
    "print(f'Stems: {stemming}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_stemmer qui prend une phrase en entrée, utilise SpaCy pour effectuer le stemming, et renvoie les stems en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_stemmer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Stemming', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'reducing', 'words', 'to', 'their', 'base', 'form', '.']\n",
      "\n",
      "\n",
      "Stems:['stem', 'is', 'an', 'essenti', 'part', 'of', 'natur', 'languag', 'process', '.', 'it', 'involv', 'reduc', 'word', 'to', 'their', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "# Fonction de stemming personnalisée avec SpaCy\n",
    "\n",
    "def custom_spacy_tokenizer(text):\n",
    "    return [s_stemmer.stem(str(word)).lower() for word in nlp(text)]\n",
    "\n",
    "print(f'Original words: {original}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(f'Stems:{custom_spacy_tokenizer(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1 \n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer la lemmatisation sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Lemmatization', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'reducing', 'words', 'to', 'their', 'base', 'form', '.']\n",
      "\n",
      "\n",
      "Lemmas:['lemmatization', 'be', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', '-pron-', 'involve', 'reduce', 'word', 'to', '-pron-', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "# Fonction de lemmatisation personnalisée avec SpaCy\n",
    "text = \"Lemmatization is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "\n",
    "sent = nlp(text)\n",
    "original = [str(word) for word in sent]\n",
    "print(f'Original words: {original}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "lemmas = [str(word.lemma_).lower() for word in nlp(text)]\n",
    "print(f'Lemmas:{lemmas}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_lemmatizer qui prend une phrase en entrée, utilise SpaCy pour effectuer la lemmatisation, et renvoie les lemmes en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_lemmatizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Lemmatization', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'reducing', 'words', 'to', 'their', 'base', 'form', '.']\n",
      "\n",
      "\n",
      "Lemmas:['lemmatization', 'be', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', '-pron-', 'involve', 'reduce', 'word', 'to', '-pron-', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "# Fonction de lemmatisation personnalisée avec SpaCy\n",
    "text = \"Lemmatization is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "\n",
    "def custom_spacy_lemmatizer(text):\n",
    "    return [str(word.lemma_).lower() for word in nlp(text)]\n",
    "\n",
    "sent = nlp(text)\n",
    "original = [str(word) for word in sent]\n",
    "print(f'Original words: {original}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(f'Lemmas:{custom_spacy_lemmatizer(text)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour filtrer les stopwords de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Stopwords', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', 'during', 'text', 'preprocessing', '.']\n",
      "Filtered words (without stopwords): [Stopwords, common, words, removed, text, preprocessing, .]\n",
      "SpaCy words: {'how', 'here', 'other', 'various', 'hers', 'to', 'nevertheless', 'several', 'ten', 'made', 'yourself', 'are', 'cannot', 'those', 'be', 'move', 'myself', 'quite', 'get', 'his', 'what', 'and', 'herein', 'since', 'thru', 'beyond', 'my', 'whenever', 'you', 'show', 'six', 'such', 'is', 'elsewhere', 'twenty', 'upon', 'whether', 'would', 'perhaps', 'formerly', 'herself', 'ever', 'per', 'between', 'empty', 'seems', 'alone', 'less', 'until', 'once', 'whereby', 'of', 'whereupon', 'say', 'see', 'so', 'himself', 'she', 'from', 'being', 'already', 'i', 'not', 'sixty', 'out', 'am', 'neither', 'although', 'hereafter', 're', 'under', 'your', 'an', 'own', 'seem', 'without', 'off', 'everyone', 'because', 'me', 'however', 'used', 'whole', 'was', 'becomes', 'enough', 'fifteen', 'any', 'give', 'though', 'next', 'forty', 'has', 'this', 'against', 'been', 'too', 'well', 'noone', 'otherwise', 'more', 'they', 'anyway', 'eleven', 'every', 'mostly', 'into', 'others', 'them', 'very', 'anyhow', 'mine', 'towards', 'before', 'go', 'that', 'thereby', 'a', 'their', 'wherever', 'after', 'beforehand', 'or', 'did', 'most', 'top', 'at', 'keep', 'put', 'full', 'amongst', 'these', 'wherein', 'least', 'much', 'ca', 'over', 'everything', 'done', 'sometime', 'beside', 'due', 'themselves', 'part', 'nine', 'namely', 'somehow', 'than', 'sometimes', 'same', 'together', 'him', 'whence', 'latterly', 'really', 'name', 'around', 'throughout', 'across', 'also', 'where', 'which', 'three', 'anything', 'else', 'serious', 'take', 'yourselves', 'few', 'toward', 'never', 'all', 'along', 'but', 'someone', 'could', 'have', 'side', 'whom', 'former', 'if', 'then', 'thereupon', 'please', 'make', 'either', 'does', 'can', 'somewhere', 'another', 'nowhere', 'we', 'in', 'thus', 'besides', 'hereby', 'often', 'he', 'everywhere', 'first', 'yet', 'will', 'onto', 'again', 'four', 'thence', 'just', 'anyone', 'as', 'amount', 'may', 'nobody', 'therein', 'become', 'five', 'still', 'becoming', 'nor', 'even', 'below', 'both', 'her', 'whatever', 'whose', 'with', 'fifty', 'thereafter', 'regarding', 'no', 'two', 'nothing', 'each', 'using', 'last', 'became', 'must', 'call', 'some', 'who', 'among', 'indeed', 'do', 'one', 'during', 'ourselves', 'whereas', 'down', 'something', 'front', 'except', 'hence', 'should', 'whither', 'back', 'about', 'eight', 'moreover', 'only', 'anywhere', 'its', 'none', 'on', 'via', 'afterwards', 'were', 'whoever', 'now', 'the', 'ours', 'itself', 'had', 'almost', 'meanwhile', 'through', 'doing', 'seemed', 'up', 'when', 'it', 'seeming', 'by', 'therefore', 'third', 'there', 'why', 'for', 'bottom', 'behind', 'further', 'yours', 'rather', 'while', 'hereupon', 'latter', 'above', 'always', 'unless', 'us', 'within', 'might', 'twelve', 'many', 'whereafter', 'our', 'hundred'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Téléchargement du modèle de langue anglaise\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Création d'une phrase complexe\n",
    "text = \"Stopwords are common words that are often removed during text preprocessing.\"\n",
    "sent = nlp(text)\n",
    "original = [str(word) for word in sent]\n",
    "\n",
    "filtered = [word for word in sent if not word.is_stop]\n",
    "\n",
    "print(f'Original words: {original}')\n",
    "print(f'Filtered words (without stopwords): {filtered}')\n",
    "print(f'SpaCy words: {nlp.Defaults.stop_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction remove_stopwords qui prend une phrase en entrée, utilise SpaCy pour filtrer les stopwords, et renvoie les mots restants en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction remove_stopwords à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Stopwords', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', 'during', 'text', 'preprocessing', '.']\n",
      "\n",
      "\n",
      "Filtered words (without stopwords): [Stopwords, common, words, removed, text, preprocessing, .]\n"
     ]
    }
   ],
   "source": [
    "# Création d'une phrase complexe\n",
    "text = \"Stopwords are common words that are often removed during text preprocessing.\"\n",
    "sent = nlp(text)\n",
    "original = [str(word) for word in sent]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [str(word) for word in nlp(sent) if not word.is_stop]\n",
    "\n",
    "print(f'Original words: {original}')\n",
    "print(f'\\n')\n",
    "print(f'Filtered words (without stopwords): {filtered}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codez une fonction 'preprocess_text_spacy' qui applique les méthodes de preprocessing vues en cours et permet d'obtenir les résultats prétraités suivants :\n",
    "\n",
    "NB : n'oublier pas de traiter les caractères spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte brut: Hello, I know that Natural Language Processing is a fascinating field!!!\n",
      "Texte prétraité avec SpaCy: hello know natural language processing fascinating field\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"Hello, I know that Natural Language Processing is a fascinating field!!!\"\n",
    "nlp.Defaults.stop_words.add('-pron-')\n",
    "import re\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "\n",
    "    preprocessed_text_lemm = [str(word.lemma_).lower() for word in nlp(text)]\n",
    "    preprocessed_text_stop = [word for word in preprocessed_text_lemm if word not in nlp.Defaults.stop_words]\n",
    "\n",
    "    return ' '.join(preprocessed_text_stop)\n",
    "\n",
    "print(f'Texte brut: {raw_text}')\n",
    "print(f'Texte prétraité avec SpaCy: {preprocess_text_spacy(raw_text)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
